{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a354c097-e4e2-4a56-b06a-1e9f8375cec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"openai/whisper-large-v2\"\n",
    "model_dir = \"models/whisper-large-v2-asr-int8\"\n",
    "\n",
    "language = \"Chinese (China)\"\n",
    "language_abbr = \"zh-CN\"\n",
    "# language_abbr = \"chinese\"\n",
    "task = \"transcribe\"\n",
    "dataset_name = \"mozilla-foundation/common_voice_11_0\"\n",
    "\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "edae1608-8621-4593-b829-c73d1a909a93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'client_id': '95368aab163e0387e4fd4991b4f2d8ccfbd4364bf656c860230501fd27dcedf087773e4695a6cf5de9c4f1d406d582283190d065cdfa36b0e2b060cffaca977e',\n",
       " 'path': '/root/.cache/huggingface/datasets/downloads/extracted/dcc5967c754d4c815fc005d6e297d84537028996cbcf6b34190517630cbc40b4/zh-CN_train_0/common_voice_zh-CN_33211332.mp3',\n",
       " 'audio': {'path': '/root/.cache/huggingface/datasets/downloads/extracted/dcc5967c754d4c815fc005d6e297d84537028996cbcf6b34190517630cbc40b4/zh-CN_train_0/common_voice_zh-CN_33211332.mp3',\n",
       "  'array': array([-6.82121026e-13, -2.27373675e-12, -2.27373675e-12, ...,\n",
       "          1.21667399e-05,  3.23003678e-06, -2.43066324e-07], shape=(255744,)),\n",
       "  'sampling_rate': 48000},\n",
       " 'sentence': '性喜温暖润湿气候且耐寒。',\n",
       " 'up_votes': 2,\n",
       " 'down_votes': 0,\n",
       " 'age': '',\n",
       " 'gender': '',\n",
       " 'accent': '',\n",
       " 'locale': 'zh-CN',\n",
       " 'segment': ''}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "common_voice = DatasetDict()\n",
    "\n",
    "common_voice[\"train\"] = load_dataset(dataset_name, language_abbr, split=\"train\", trust_remote_code=True)\n",
    "common_voice[\"validation\"] = load_dataset(dataset_name, language_abbr, split=\"validation\", trust_remote_code=True)\n",
    "\n",
    "common_voice[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a7891554-abf5-4356-bb1e-f1b2a9c050fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n",
       "        num_rows: 29056\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n",
       "        num_rows: 10581\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0a63938c-d2cb-4d96-8814-f09a80e217a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoFeatureExtractor, AutoTokenizer, AutoProcessor\n",
    "\n",
    "# 从预训练模型加载特征提取器\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_name_or_path)\n",
    "\n",
    "# 从预训练模型加载分词器，可以指定语言和任务以获得最适合特定需求的分词器配置\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, language=language, task=task)\n",
    "\n",
    "# 从预训练模型加载处理器，处理器通常结合了特征提取器和分词器，为特定任务提供一站式的数据预处理\n",
    "processor = AutoProcessor.from_pretrained(model_name_or_path, language=language, task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "17776351-f271-4628-a062-10f0fc32ed96",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice = common_voice.remove_columns(\n",
    "    [\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d3042fa3-dd4f-4777-9114-6eaf40f9746e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': {'path': '/root/.cache/huggingface/datasets/downloads/extracted/dcc5967c754d4c815fc005d6e297d84537028996cbcf6b34190517630cbc40b4/zh-CN_train_0/common_voice_zh-CN_33211332.mp3',\n",
       "  'array': array([-6.82121026e-13, -2.27373675e-12, -2.27373675e-12, ...,\n",
       "          1.21667399e-05,  3.23003678e-06, -2.43066324e-07], shape=(255744,)),\n",
       "  'sampling_rate': 48000},\n",
       " 'sentence': '性喜温暖润湿气候且耐寒。'}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_voice[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cc655af3-0d03-4899-8300-63f8ea95db9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Audio\n",
    "\n",
    "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "37f69197-9383-4d3c-a7e6-992118e26976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': {'path': '/root/.cache/huggingface/datasets/downloads/extracted/dcc5967c754d4c815fc005d6e297d84537028996cbcf6b34190517630cbc40b4/zh-CN_train_0/common_voice_zh-CN_33211332.mp3',\n",
       "  'array': array([ 5.09317033e-11, -7.27595761e-12, -6.54836185e-11, ...,\n",
       "         -5.96661994e-06,  2.71382887e-05,  1.29687978e-05], shape=(85248,)),\n",
       "  'sampling_rate': 16000},\n",
       " 'sentence': '性喜温暖润湿气候且耐寒。'}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sampling_rate 从 48KHZ 降为 16KHZ\n",
    "common_voice[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "aac92b40-dfd4-40fb-9178-326fd0d5954b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "04be2eae-e2f1-4721-9b69-b5e97d229d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'sentence'],\n",
       "        num_rows: 640\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['audio', 'sentence'],\n",
       "        num_rows: 320\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_common_voice = DatasetDict()\n",
    "\n",
    "small_common_voice[\"train\"] = common_voice[\"train\"].shuffle(seed=16).select(range(640))\n",
    "small_common_voice[\"validation\"] = common_voice[\"validation\"].shuffle(seed=16).select(range(320))\n",
    "\n",
    "small_common_voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5725733c-7ad2-4ed0-b101-76a0c26e0962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 抽样数据处理\n",
    "tokenized_common_voice = small_common_voice.map(prepare_dataset)\n",
    "\n",
    "# 完整数据训练，尝试开启 `num_proc=8` 参数多进程并行处理（如阻塞无法运行，则不使用此参数）\n",
    "# tokenized_common_voice = common_voice.map(prepare_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "254539e8-9e6a-4916-b668-4358c9861fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "# 定义一个针对语音到文本任务的数据整理器类\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any  # 处理器结合了特征提取器和分词器\n",
    "\n",
    "    # 整理器函数，将特征列表处理成一个批次\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # 从特征列表中提取输入特征，并填充以使它们具有相同的形状\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # 从特征列表中提取标签特征（文本令牌），并进行填充\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # 使用-100替换标签中的填充区域，-100通常用于在损失计算中忽略填充令牌\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # 如果批次中的所有序列都以句子开始令牌开头，则移除它\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        # 将处理过的标签添加到批次中\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch  # 返回最终的批次，准备好进行训练或评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4c1ce43b-3350-4a9b-aa78-ad009d576fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用给定的处理器实例化数据整理器\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f49991ab-de45-4a38-affc-e8d9a305fd09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.whisper.modeling_whisper.WhisperForConditionalGeneration'>\n"
     ]
    }
   ],
   "source": [
    "# 1. 强制使用正确的模型类（WhisperForConditionalGeneration）\n",
    "from transformers import WhisperForConditionalGeneration, BitsAndBytesConfig\n",
    "\n",
    "# 2. 8位量化配置不变\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(model_name_or_path, load_in_8bit=True, device_map=\"auto\")\n",
    "\n",
    "\n",
    "\n",
    "# 验证模型类型（可选，用于确认是否正确加载）\n",
    "print(type(model))  # 应输出：<class 'transformers.models.whisper.modeling_whisper.WhisperForConditionalGeneration'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "58af9053-8c63-4bd5-983a-d8262bfa6e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 设置模型配置中的forced_decoder_ids属性为None\n",
    "# model.config.forced_decoder_ids = None  # 这通常用于指定在解码（生成文本）过程中必须使用的特定token的ID，设置为None表示没有这样的强制要求\n",
    "\n",
    "# # 设置模型配置中的suppress_tokens列表为空\n",
    "# model.config.suppress_tokens = []  # 这用于指定在生成过程中应被抑制（不生成）的token的列表，设置为空列表表示没有要抑制的token\n",
    "\n",
    "\n",
    "# 配置解码器提示ID（适配语音转文本任务）\n",
    "model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(\n",
    "    language=\"chinese\",  # 用全称，而非缩写\n",
    "    task=task\n",
    ")\n",
    "model.config.suppress_tokens = []  # 不抑制任何token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d3a0cb9f-8216-4556-8e35-4fdbf22d7730",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSpeechSeq2Seq, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# 配置8位量化参数\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,  # 保持8位量化\n",
    "    bnb_8bit_compute_dtype=torch.float16,  # 计算时使用float16\n",
    "    bnb_8bit_use_double_quant=True,  # 双量化优化内存\n",
    "    bnb_8bit_quant_type=\"nf8\"  # 8位量化的优化类型\n",
    ")\n",
    "\n",
    "# 加载模型时应用量化配置，device_map=\"auto\"会自动分配设备\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    quantization_config=bnb_config,  # 应用8位量化配置\n",
    "    device_map=\"auto\"  # 自动分配设备，无需后续手动移动\n",
    ")\n",
    "\n",
    "# 获取模型所在设备（由device_map自动分配）\n",
    "device = model.device\n",
    "\n",
    "# 确保卷积层参数类型正确（针对Whisper的conv1/conv2）\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Conv1d):\n",
    "        # 强制卷积层权重和偏置为float16（与输入匹配）\n",
    "        # 不移动设备，只转换类型\n",
    "        module.weight = torch.nn.Parameter(module.weight.half())\n",
    "        if module.bias is not None:\n",
    "            module.bias = torch.nn.Parameter(module.bias.half())\n",
    "\n",
    "# 处理输入数据类型 - 在使用时调用\n",
    "def prepare_inputs(input_features):\n",
    "    # 将输入转换为float16并移动到模型所在设备\n",
    "    return input_features.half().to(device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c28e3c38-8a15-42cd-8753-73c1a89c1a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, PeftModel, LoraModel, LoraConfig, get_peft_model\n",
    "\n",
    "# 创建一个LoraConfig对象，用于设置LoRA（Low-Rank Adaptation）的配置参数\n",
    "config = LoraConfig(\n",
    "    r=4,  # LoRA的秩，影响LoRA矩阵的大小\n",
    "    lora_alpha=64,  # LoRA适应的比例因子\n",
    "    # 指定将LoRA应用到的模型模块，通常是attention和全连接层的投影。\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,  # 在LoRA模块中使用的dropout率\n",
    "    bias=\"none\",  # 设置bias的使用方式，这里没有使用bias\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d313f240-f3a9-403f-8575-e4a14c389b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2ac007ea-39d3-492a-af79-4158b82c2bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,966,080 || all params: 1,545,271,040 || trainable%: 0.1272\n"
     ]
    }
   ],
   "source": [
    "# 打印 LoRA 微调训练的模型参数\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e95d61c3-d913-4aad-bdd4-c4d32c5477b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "# 设置序列到序列模型训练的参数\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=model_dir,  # 指定模型输出和保存的目录\n",
    "    per_device_train_batch_size=batch_size,  # 每个设备上的训练批量大小\n",
    "    learning_rate=1e-3,  # 学习率\n",
    "    num_train_epochs=1,  # 训练的总轮数\n",
    "    eval_strategy=\"epoch\",  # 新版本中使用eval_strategy替代evaluation_strategy\n",
    "    per_device_eval_batch_size=batch_size,  # 每个设备上的评估批量大小\n",
    "    generation_max_length=128,  # 生成任务的最大长度\n",
    "    logging_steps=10,  # 指定日志记录的步骤，用于跟踪训练进度\n",
    "    remove_unused_columns=False,  # 是否删除不使用的列，以减少数据处理开销\n",
    "    label_names=[\"labels\"],  # 指定标签列的名称，用于训练过程中\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d95a76ee-e098-4eac-9aab-eaa3e135a451",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_530564/3403953682.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=peft_model,\n",
    "    train_dataset=tokenized_common_voice[\"train\"],\n",
    "    eval_dataset=tokenized_common_voice[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")\n",
    "peft_model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0a2a0e68-d3ec-489f-bcf1-ac1a8895ec9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.whisper.modeling_whisper.WhisperForConditionalGeneration'>\n"
     ]
    }
   ],
   "source": [
    "print(type(model))  # 应输出：<class 'transformers.models.whisper.modeling_whisper.WhisperForConditionalGeneration'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e93f28a-7c24-449d-aca9-5787f27d941e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载全量数据集...\n",
      "预处理全量数据（多进程加速）...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b417b521936437bb462e8579cf1b777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/29056 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset, DatasetDict, Audio\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    AutoModelForSpeechSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "import evaluate\n",
    "import os\n",
    "\n",
    "# 中文显示配置\n",
    "plt.rcParams[\"font.family\"] = [\"SimHei\", \"WenQuanYi Micro Hei\", \"Heiti TC\"]\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "# -------------- 关键加速配置（必须依赖高端GPU）--------------\n",
    "model_name_or_path = \"openai/whisper-large-v2\"  # 保留大模型\n",
    "batch_size = 64  # 超大批次（需24GB+显存）\n",
    "gradient_accumulation_steps = 1  # 不累积梯度，直接大批次\n",
    "num_train_epochs = 1  # 仅训练1轮（全量数据）\n",
    "learning_rate = 3e-3  # 高学习率加速收敛\n",
    "logging_steps = 20  # 减少日志频率\n",
    "eval_steps = 200  # 减少验证频率\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "output_dir = \"./whisper_large_fast\"\n",
    "language = \"Chinese\"\n",
    "language_abbr = \"zh-CN\"\n",
    "task = \"transcribe\"\n",
    "dataset_name = \"mozilla-foundation/common_voice_11_0\"\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 1. 加载全量数据集（保留全量数据）\n",
    "print(\"加载全量数据集...\")\n",
    "common_voice = DatasetDict()\n",
    "common_voice[\"train\"] = load_dataset(dataset_name, language_abbr, split=\"train\", trust_remote_code=True)\n",
    "common_voice[\"validation\"] = load_dataset(dataset_name, language_abbr, split=\"validation\", trust_remote_code=True)\n",
    "common_voice[\"test\"] = load_dataset(dataset_name, language_abbr, split=\"test\", trust_remote_code=True)\n",
    "\n",
    "# 移除冗余字段加速处理\n",
    "common_voice = common_voice.remove_columns(\n",
    "    [\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"]\n",
    ")\n",
    "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "# 2. 加载处理器（复用大模型处理器）\n",
    "processor = AutoProcessor.from_pretrained(model_name_or_path, language=language, task=task)\n",
    "feature_extractor = processor.feature_extractor\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "# 3. 预处理（多进程+精简逻辑）\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=16000).input_features[0]\n",
    "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "print(\"预处理全量数据（多进程加速）...\")\n",
    "tokenized_common_voice = common_voice.map(\n",
    "    prepare_dataset,\n",
    "    remove_columns=common_voice[\"train\"].column_names,\n",
    "    num_proc=16  # 最大程度利用CPU核心\n",
    ")\n",
    "\n",
    "# 4. 数据整理器（复用逻辑）\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    def __call__(self, features):\n",
    "        input_features = [{\"input_features\": f[\"input_features\"]} for f in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "        \n",
    "        label_features = [{\"input_ids\": f[\"labels\"]} for f in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        \n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
    "\n",
    "# 5. 评估指标（仅保留WER）\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    return {\"wer\": 100 * wer_metric.compute(predictions=pred_str, references=label_str)}\n",
    "\n",
    "# 6. 模型配置（4bit量化+LoRA极致加速）\n",
    "print(\"加载whisper-large-v2（4bit量化）...\")\n",
    "# 4bit量化（比8bit快2倍，显存占用减少50%）\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16  # 计算精度\n",
    ")\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",  # 自动分配到GPU\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# 关键：冻结大部分参数，仅训练LoRA（加速5-10倍）\n",
    "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)  # 梯度检查点省显存\n",
    "\n",
    "# LoRA配置（仅适配关键模块）\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # 仅注意力Q/V投影层\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(\"可训练参数:\")\n",
    "model.print_trainable_parameters()  # 仅0.05%参数可训练\n",
    "\n",
    "# 7. 训练参数（极限加速配置）\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    logging_steps=logging_steps,\n",
    "    eval_steps=eval_steps,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"no\",  # 不保存中间模型（省时间）\n",
    "    load_best_model_at_end=False,\n",
    "    fp16=True,  # 强制FP16\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=\"paged_adamw_8bit\",  # 8bit优化器（省显存）\n",
    "    generation_max_length=128,\n",
    "    remove_unused_columns=False,\n",
    "    label_names=[\"labels\"],\n",
    "    report_to=\"none\",  # 关闭日志报告\n",
    "    dataloader_num_workers=8,\n",
    "    dataloader_pin_memory=True,  # 内存pin住加速数据传输\n",
    ")\n",
    "\n",
    "# 8. 自定义训练器（确保参数正确）\n",
    "class CustomTrainer(Seq2SeqTrainer):\n",
    "    def training_step(self, model, inputs, num_items_in_batch):\n",
    "        outputs = model(input_features=inputs[\"input_features\"], labels=inputs[\"labels\"])\n",
    "        loss = outputs.loss\n",
    "        self.accelerator.backward(loss)\n",
    "        return loss.detach()\n",
    "\n",
    "# 9. 开始训练\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_common_voice[\"train\"],\n",
    "    eval_dataset=tokenized_common_voice[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    processing_class=processor\n",
    ")\n",
    "\n",
    "print(\"开始训练（依赖高端GPU，预计1-2小时）...\")\n",
    "train_result = trainer.train()\n",
    "\n",
    "# 10. 绘制损失曲线\n",
    "logs = trainer.state.log_history\n",
    "train_losses = [log[\"loss\"] for log in logs if \"loss\" in log and \"eval_loss\" not in log]\n",
    "train_steps = [log[\"step\"] for log in logs if \"loss\" in log and \"eval_loss\" not in log]\n",
    "val_losses = [log[\"eval_loss\"] for log in logs if \"eval_loss\" in log]\n",
    "val_steps = [log[\"step\"] for log in logs if \"eval_loss\" in log]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(train_steps, train_losses, label=\"训练损失\")\n",
    "plt.plot(val_steps, val_losses, label=\"验证损失\")\n",
    "plt.xlabel(\"训练步数\")\n",
    "plt.ylabel(\"损失值\")\n",
    "plt.title(\"训练与验证损失变化\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"{output_dir}/loss_curve.png\")\n",
    "\n",
    "# 11. 测试集评估\n",
    "test_results = trainer.evaluate(eval_dataset=tokenized_common_voice[\"test\"])\n",
    "print(f\"测试集WER: {test_results['wer']:.2f}%\")\n",
    "\n",
    "print(f\"训练完成，结果保存至 {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd2badb-368a-436f-9ef9-eb9ef7ae4e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#以上代码导致：服务器磁盘读写/内存出现了跑高的情况，主要是磁盘读写/内存的读占用过高导致系统夯住了，导致系统无法远程连接。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b7d477-a850-4556-8115-2f0a9a6d4942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset, DatasetDict, Audio\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    AutoModelForSpeechSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "import evaluate\n",
    "import os\n",
    "import gc  # 垃圾回收\n",
    "\n",
    "# 中文显示配置\n",
    "plt.rcParams[\"font.family\"] = [\"SimHei\", \"WenQuanYi Micro Hei\", \"Heiti TC\"]\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "\n",
    "# -------------- 低资源配置（减少磁盘/内存压力）--------------\n",
    "model_name_or_path = \"openai/whisper-large-v2\"\n",
    "batch_size = 16  # 降低批次，减少内存占用\n",
    "gradient_accumulation_steps = 4  # 梯度累积替代大批次，平衡显存\n",
    "num_train_epochs = 1\n",
    "learning_rate = 2e-3\n",
    "logging_steps = 50\n",
    "eval_steps = 500\n",
    "max_preprocess_workers = 4  # 降低预处理并行度，减少磁盘IO\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "output_dir = \"./whisper_large_low_resource\"\n",
    "language = \"Chinese\"\n",
    "language_abbr = \"zh-CN\"\n",
    "task = \"transcribe\"\n",
    "dataset_name = \"mozilla-foundation/common_voice_11_0\"\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 1. 分阶段加载数据集（减少一次性内存占用）\n",
    "print(\"分阶段加载数据集...\")\n",
    "common_voice = DatasetDict()\n",
    "\n",
    "# 训练集分块加载（避免一次性读入内存）\n",
    "train_dataset = load_dataset(\n",
    "    dataset_name, language_abbr, split=\"train\", trust_remote_code=True, streaming=False\n",
    ")\n",
    "common_voice[\"train\"] = train_dataset\n",
    "\n",
    "# 验证集和测试集正常加载\n",
    "common_voice[\"validation\"] = load_dataset(\n",
    "    dataset_name, language_abbr, split=\"validation\", trust_remote_code=True\n",
    ")\n",
    "common_voice[\"test\"] = load_dataset(\n",
    "    dataset_name, language_abbr, split=\"test\", trust_remote_code=True\n",
    ")\n",
    "\n",
    "# 移除冗余字段，减少数据体积\n",
    "common_voice = common_voice.remove_columns(\n",
    "    [\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"]\n",
    ")\n",
    "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "# 手动触发垃圾回收，释放未使用内存\n",
    "gc.collect()\n",
    "\n",
    "# 2. 加载处理器（复用）\n",
    "processor = AutoProcessor.from_pretrained(model_name_or_path, language=language, task=task)\n",
    "feature_extractor = processor.feature_extractor\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "# 3. 低IO预处理（减少并行度+分块处理）\n",
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    # 音频特征提取（避免临时文件堆积）\n",
    "    batch[\"input_features\"] = feature_extractor(\n",
    "        audio[\"array\"], sampling_rate=16000, return_tensors=\"pt\"  # 直接返回Tensor，减少中间转换\n",
    "    ).input_features[0]\n",
    "    # 标签处理\n",
    "    batch[\"labels\"] = tokenizer(batch[\"sentence\"], truncation=True, max_length=128).input_ids\n",
    "    return batch\n",
    "\n",
    "print(\"低并行度预处理数据...\")\n",
    "tokenized_common_voice = common_voice.map(\n",
    "    prepare_dataset,\n",
    "    remove_columns=common_voice[\"train\"].column_names,\n",
    "    num_proc=max_preprocess_workers,  # 降低并行进程数，减少磁盘读写\n",
    "    batch_size=100,  # 分小批处理，避免内存峰值\n",
    "    load_from_cache_file=True,  # 缓存预处理结果，避免重复IO\n",
    "    cache_file_names={  # 自定义缓存路径，集中管理\n",
    "        \"train\": f\"{output_dir}/train_cache.arrow\",\n",
    "        \"validation\": f\"{output_dir}/val_cache.arrow\",\n",
    "        \"test\": f\"{output_dir}/test_cache.arrow\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# 再次回收内存\n",
    "gc.collect()\n",
    "\n",
    "# 4. 轻量数据整理器（避免冗余操作）\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    def __call__(self, features):\n",
    "        # 仅处理必要字段，减少中间变量\n",
    "        input_features = [{\"input_features\": f[\"input_features\"]} for f in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "        \n",
    "        label_features = [{\"input_ids\": f[\"labels\"]} for f in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        \n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
    "\n",
    "# 5. 评估指标（延迟加载，减少初始化内存）\n",
    "def compute_metrics(pred):\n",
    "    wer_metric = evaluate.load(\"wer\")  # 评估时才加载，节省内存\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    return {\"wer\": 100 * wer_metric.compute(predictions=pred_str, references=label_str)}\n",
    "\n",
    "# 6. 模型配置（平衡速度和资源）\n",
    "print(\"加载模型（8bit量化，低内存模式）...\")\n",
    "# 8bit量化（比4bit更稳定，内存占用适中）\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_use_double_quant=True,\n",
    "    bnb_8bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True  # 启用低CPU内存模式\n",
    ")\n",
    "\n",
    "# 配置模型，减少不必要计算\n",
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)  # 梯度检查点省显存\n",
    "\n",
    "# LoRA配置（适度适配，避免过多计算）\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(\"可训练参数:\")\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# 回收模型加载后的冗余内存\n",
    "gc.collect()\n",
    "\n",
    "# 7. 训练参数（减少IO操作）\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    logging_steps=logging_steps,\n",
    "    eval_steps=eval_steps,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,  # 减少保存频率，降低磁盘IO\n",
    "    save_total_limit=1,  # 只保留1个模型文件\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=\"paged_adamw_8bit\",  # 分页优化器，减少内存碎片\n",
    "    generation_max_length=128,\n",
    "    remove_unused_columns=False,\n",
    "    label_names=[\"labels\"],\n",
    "    report_to=\"none\",\n",
    "    dataloader_num_workers=2,  # 降低数据加载线程，减少磁盘压力\n",
    "    dataloader_pin_memory=False,  # 关闭pin内存，避免内存锁定\n",
    "    disable_tqdm=False,  # 保留进度条，方便观察状态\n",
    ")\n",
    "\n",
    "# 8. 自定义训练器（定期回收内存）\n",
    "class CustomTrainer(Seq2SeqTrainer):\n",
    "    def training_step(self, model, inputs, num_items_in_batch):\n",
    "        outputs = model(input_features=inputs[\"input_features\"], labels=inputs[\"labels\"])\n",
    "        loss = outputs.loss\n",
    "        self.accelerator.backward(loss)\n",
    "        # 每10步回收一次内存，避免内存泄漏\n",
    "        if self.state.global_step % 10 == 0:\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()  # 清空CUDA缓存\n",
    "        return loss.detach()\n",
    "\n",
    "# 9. 开始训练\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_common_voice[\"train\"],\n",
    "    eval_dataset=tokenized_common_voice[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    processing_class=processor\n",
    ")\n",
    "\n",
    "print(\"开始训练（低资源模式，预计2-3小时）...\")\n",
    "train_result = trainer.train()\n",
    "\n",
    "# 10. 绘制损失曲线（轻量化）\n",
    "logs = trainer.state.log_history\n",
    "train_losses = [log[\"loss\"] for log in logs if \"loss\" in log and \"eval_loss\" not in log]\n",
    "train_steps = [log[\"step\"] for log in logs if \"loss\" in log and \"eval_loss\" not in log]\n",
    "val_losses = [log[\"eval_loss\"] for log in logs if \"eval_loss\" in log]\n",
    "val_steps = [log[\"step\"] for log in logs if \"eval_loss\" in log]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(train_steps, train_losses, label=\"训练损失\")\n",
    "plt.plot(val_steps, val_losses, label=\"验证损失\")\n",
    "plt.xlabel(\"训练步数\")\n",
    "plt.ylabel(\"损失值\")\n",
    "plt.title(\"训练与验证损失变化\")\n",
    "plt.legend()\n",
    "plt.savefig(f\"{output_dir}/loss_curve.png\")\n",
    "plt.close()  # 关闭图像，释放内存\n",
    "\n",
    "# 11. 测试集评估（分批次评估，减少内存）\n",
    "test_results = trainer.evaluate(\n",
    "    eval_dataset=tokenized_common_voice[\"test\"],\n",
    "    max_length=128,\n",
    "    num_beams=1  # 简化生成策略，减少计算\n",
    ")\n",
    "print(f\"测试集WER: {test_results['wer']:.2f}%\")\n",
    "\n",
    "# 最终清理\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"训练完成，结果保存至 {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AliOpenAPI",
   "language": "python",
   "name": "aliopenapi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
